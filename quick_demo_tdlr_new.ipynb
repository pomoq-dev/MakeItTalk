{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "quick_demo_tdlr.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyPb4R1PTR2YSS24nlUOTul6",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "8f388e32c2dd4d118c3926d13a40a639": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DropdownModel",
     "state": {
      "_options_labels": [
       "angelina",
       "anne",
       "anne2",
       "audrey",
       "aya",
       "captain",
       "captain2",
       "cesi",
       "chris",
       "chris2",
       "dali",
       "donald",
       "dragonmom",
       "dwayne",
       "dwayne2",
       "dwayne3",
       "harry",
       "hermione",
       "hermione2",
       "hound",
       "jali",
       "john",
       "johncartoon",
       "johnny",
       "kalo",
       "lab1",
       "lab2",
       "lab3",
       "lab4",
       "leo",
       "leo2",
       "monalisa2",
       "monalisa3",
       "morgan",
       "mulan",
       "natalie",
       "natalie2",
       "neo",
       "obama",
       "paint1",
       "paint3",
       "paint_boy",
       "paint_boy2",
       "rihanna",
       "ron",
       "scarlett",
       "statue1",
       "statue2",
       "stephen",
       "taylor",
       "trump",
       "trump2"
      ],
      "_view_name": "DropdownView",
      "style": "IPY_MODEL_9bbc2a0602734ef7929d0ae34849b8b7",
      "_dom_classes": [],
      "description": "",
      "_model_name": "DropdownModel",
      "index": 12,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "disabled": false,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_422e4dc6267b4717a3cb075376761645"
     }
    },
    "9bbc2a0602734ef7929d0ae34849b8b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "422e4dc6267b4717a3cb075376761645": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pomoq-dev/MakeItTalk/blob/main/quick_demo_tdlr_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FpIfnReur6z",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MakeItTalk Quick Demo (natural human face animation)\n",
    "\n",
    "## TDLR version\n",
    "\n",
    "Remember to change to GPU runtime first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PZQxNvvuuNZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparation (run only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "SRC_AUDIO_DIR = \"/content/drive/MyDrive/SRC_AUDIO\"\n",
    "SRC_IMGS_DIR = \"/content/drive/MyDrive/HOBANA_RESULTS_PHOTOS2\"\n",
    "IMGS_DIR = \"/content/drive/MyDrive/IMGS_TO_WORK\"\n",
    "RES_DIR = \"/content/drive/MyDrive/RES_FACE_SPEECH\"\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "os.makedirs(IMGS_DIR, exist_ok=True)\n",
    "os.makedirs('examples/wav_files', exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "56UHwyJKuaWw",
    "outputId": "b85928a8-4556-45be-e2bb-bd2c285a3be7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "print('Git clone project and install requirements...')\n",
    "!git clone https://github.com/pomoq-dev/MakeItTalk &> /dev/null\n",
    "%cd MakeItTalk/\n",
    "!export PYTHONPATH=/content/MakeItTalk:$PYTHONPATH\n",
    "!pip install -r requirements.txt &> /dev/null\n",
    "!pip install tensorboardX &> /dev/null\n",
    "!pip install gdown\n",
    "!mkdir examples/dump\n",
    "!mkdir examples/ckpt\n",
    "!pip install gdown &> /dev/null\n",
    "print('Done!')\n",
    "print('Download pre-trained models...')\n",
    "!gdown -O examples/ckpt/ckpt_autovc.pth https://drive.google.com/uc?id=1ZiwPp_h62LtjU0DwpelLUoodKPR85K7x\n",
    "!gdown -O examples/ckpt/ckpt_content_branch.pth https://drive.google.com/uc?id=1r3bfEvTVl6pCNw5xwUhEglwDHjWtAqQp\n",
    "!gdown -O examples/ckpt/ckpt_speaker_branch.pth https://drive.google.com/uc?id=1rV0jkyDqPW-aDJcj7xSO6Zt1zSXqn1mu\n",
    "!gdown -O examples/ckpt/ckpt_116_i2i_comb.pth https://drive.google.com/uc?id=1i2LJXKp-yWKIEEgJ7C6cE3_2NirfY_0a\n",
    "!gdown -O examples/dump/emb.pickle https://drive.google.com/uc?id=18-0CYl5E6ungS3H4rRSHjfYvvm-WwjTI\n",
    "print('Done!')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlEgsX1Ivzbm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Animate your photos here!\n",
    "\n",
    "- Upload your images to `examples` with size `256x256`. Or use existing ones.\n",
    "\n",
    "- Upload your speech audio files (could be many) to `examples`. Since it will process all `.wav` files under `examples`, remember to delete non-necessary `.wav` files. Or use an existing one `M6_04_16k.wav`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bF2RPbCEM2I",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 2/3: Setup your animation controllers (on right Sliders)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hzeK4TPzy_82",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "#@markdown # Animation Controllers\n",
    "#@markdown Amplify the lip motion in horizontal direction\n",
    "AMP_LIP_SHAPE_X = 2 #@param {type:\"slider\", min:0.5, max:5.0, step:0.1}\n",
    "\n",
    "#@markdown Amplify the lip motion in vertical direction\n",
    "AMP_LIP_SHAPE_Y = 2 #@param {type:\"slider\", min:0.5, max:5.0, step:0.1}\n",
    "\n",
    "#@markdown Amplify the head pose motion (usually smaller than 1.0, put it to 0. for a static head pose)\n",
    "AMP_HEAD_POSE_MOTION = 0.35 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "\n",
    "#@markdown Add naive eye blink\n",
    "ADD_NAIVE_EYE = True  #@param [\"False\", \"True\"] {type:\"raw\"}\n",
    "\n",
    "#@markdown If your image has an opened mouth, put this as True, else False\n",
    "CLOSE_INPUT_FACE_MOUTH = False  #@param [\"False\", \"True\"] {type:\"raw\"}          \n",
    "\n",
    "\n",
    "#@markdown # Landmark Adjustment\n",
    "\n",
    "#@markdown Adjust upper lip thickness (postive value means thicker)\n",
    "UPPER_LIP_ADJUST = -1 #@param {type:\"slider\", min:-3.0, max:3.0, step:1.0}\n",
    "\n",
    "#@markdown Adjust lower lip thickness (postive value means thicker)\n",
    "LOWER_LIP_ADJUST = -1 #@param {type:\"slider\", min:-3.0, max:3.0, step:1.0}\n",
    "\n",
    "#@markdown Adjust static lip width (in multipication)\n",
    "LIP_WIDTH_ADJUST = 1.0 #@param {type:\"slider\", min:0.8, max:1.2, step:0.01}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0KE1rLxB_Ce",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 3/3: One-click to Run (just wait in seconds)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XFQh-tlKzQCm",
    "outputId": "617a2c4d-f8cf-4f44-a2a6-5dcc92673302",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append(\"thirdparty/AdaptiveWingLoss\")\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import argparse\n",
    "from src.approaches.train_image_translation_new import Image_translation_block\n",
    "import torch\n",
    "import pickle\n",
    "import face_alignment\n",
    "from src.autovc.AutoVC_mel_Convertor_retrain_version import AutoVC_mel_Convertor\n",
    "import shutil\n",
    "import time\n",
    "import util.utils as util\n",
    "from scipy.signal import savgol_filter\n",
    "from src.approaches.train_audio2landmark import Audio2landmark_model\n",
    "\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "sys.stdout = open(os.devnull, 'a')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--jpg', type=str, default='{}.jpg'.format(default_head_name.value))\n",
    "parser.add_argument('--close_input_face_mouth', default=CLOSE_INPUT_FACE_MOUTH, action='store_true')\n",
    "parser.add_argument('--load_AUTOVC_name', type=str, default='examples/ckpt/ckpt_autovc.pth')\n",
    "parser.add_argument('--load_a2l_G_name', type=str, default='examples/ckpt/ckpt_speaker_branch.pth')\n",
    "parser.add_argument('--load_a2l_C_name', type=str, default='examples/ckpt/ckpt_content_branch.pth') #ckpt_audio2landmark_c.pth')\n",
    "parser.add_argument('--load_G_name', type=str, default='examples/ckpt/ckpt_116_i2i_comb.pth') #ckpt_image2image.pth') #ckpt_i2i_finetune_150.pth') #c\n",
    "parser.add_argument('--amp_lip_x', type=float, default=AMP_LIP_SHAPE_X)\n",
    "parser.add_argument('--amp_lip_y', type=float, default=AMP_LIP_SHAPE_Y)\n",
    "parser.add_argument('--amp_pos', type=float, default=AMP_HEAD_POSE_MOTION)\n",
    "parser.add_argument('--reuse_train_emb_list', type=str, nargs='+', default=[]) #  ['iWeklsXc0H8']) #['45hn7-LXDX8']) #['E_kmpT-EfOg']) #'iWeklsXc0H8', '29k8RtSUjE0', '45hn7-LXDX8',\n",
    "parser.add_argument('--add_audio_in', default=False, action='store_true')\n",
    "parser.add_argument('--comb_fan_awing', default=False, action='store_true')\n",
    "parser.add_argument('--output_folder', type=str, default='examples')\n",
    "parser.add_argument('--test_end2end', default=True, action='store_true')\n",
    "parser.add_argument('--dump_dir', type=str, default='', help='')\n",
    "parser.add_argument('--pos_dim', default=7, type=int)\n",
    "parser.add_argument('--use_prior_net', default=True, action='store_true')\n",
    "parser.add_argument('--transformer_d_model', default=32, type=int)\n",
    "parser.add_argument('--transformer_N', default=2, type=int)\n",
    "parser.add_argument('--transformer_heads', default=2, type=int)\n",
    "parser.add_argument('--spk_emb_enc_size', default=16, type=int)\n",
    "parser.add_argument('--init_content_encoder', type=str, default='')\n",
    "parser.add_argument('--lr', type=float, default=1e-3, help='learning rate')\n",
    "parser.add_argument('--reg_lr', type=float, default=1e-6, help='weight decay')\n",
    "parser.add_argument('--write', default=False, action='store_true')\n",
    "parser.add_argument('--segment_batch_size', type=int, default=1, help='batch size')\n",
    "parser.add_argument('--emb_coef', default=3.0, type=float)\n",
    "parser.add_argument('--lambda_laplacian_smooth_loss', default=1.0, type=float)\n",
    "parser.add_argument('--use_11spk_only', default=False, action='store_true')\n",
    "parser.add_argument('-f')\n",
    "opt_parser = parser.parse_args()\n",
    "\n",
    "# imgs = [cv2.imread('examples/images_faces/' + opt_parser.jpg)]\n",
    "\n",
    "# Preparing images ------------------------\n",
    "AGES = range(7, 25)\n",
    "all_src_images = os.listdir(SRC_IMGS_DIR)\n",
    "good_images_names = []\n",
    "for name in all_src_images:\n",
    "    spl = name.split('_')\n",
    "    if len(spl) > 0 and spl[0].isnumeric() and int(spl[0]) in AGES:\n",
    "        shutil.copyfile(os.path.join(SRC_IMGS_DIR, name), os.path.join(IMGS_DIR, name))\n",
    "\n",
    "from image_converter import convert_images\n",
    "convert_images(IMGS_DIR)\n",
    "\n",
    "imgs = []\n",
    "IMG_NAMES = os.listdir(IMGS_DIR)\n",
    "for img_name in IMG_NAMES:\n",
    "    if img_name.endswith('.jpg'):\n",
    "        img_path = os.path.join(IMGS_DIR, img_name)\n",
    "        imgs.append(cv2.imread(img_path))\n",
    "random.shuffle(imgs)\n",
    "# ------------------------------------------\n",
    "\n",
    "# Preparing audios ------------------------\n",
    "audio_names = os.listdir(SRC_AUDIO_DIR)\n",
    "for audio_name in audio_names:\n",
    "    if audio_name.endswith('.wav'):\n",
    "        shutil.copyfile(os.path.join(SRC_AUDIO_DIR, name), os.path.join('examples/wav_files', name))\n",
    "# -----------------------------------------\n",
    "\n",
    "for img in imgs:\n",
    "    predictor = face_alignment.FaceAlignment(face_alignment.LandmarksType._3D, device='cpu', flip_input=True)\n",
    "    shapes = predictor.get_landmarks(img)\n",
    "    if (not shapes or len(shapes) != 1):\n",
    "        print('Cannot detect face landmarks. Exit.')\n",
    "        exit(-1)\n",
    "    shape_3d = shapes[0]\n",
    "    if(opt_parser.close_input_face_mouth):\n",
    "        util.close_input_face_mouth(shape_3d)\n",
    "    shape_3d[48:, 0] = (shape_3d[48:, 0] - np.mean(shape_3d[48:, 0])) * LIP_WIDTH_ADJUST + np.mean(shape_3d[48:, 0]) # wider lips\n",
    "    shape_3d[49:54, 1] -= UPPER_LIP_ADJUST           # thinner upper lip\n",
    "    shape_3d[55:60, 1] += LOWER_LIP_ADJUST           # thinner lower lip\n",
    "    shape_3d[[37,38,43,44], 1] -=2.    # larger eyes\n",
    "    shape_3d[[40,41,46,47], 1] +=2.    # larger eyes\n",
    "    shape_3d, scale, shift = util.norm_input_face(shape_3d)\n",
    "\n",
    "    print(\"Loaded Image...\", file=sys.stderr)\n",
    "\n",
    "    au_data = []\n",
    "    au_emb = []\n",
    "    ains = glob.glob1('examples/wav_files', '*.wav')\n",
    "    ains = [item for item in ains if item is not 'tmp.wav']\n",
    "    ains.sort()\n",
    "\n",
    "    # Get 3 random audios for image\n",
    "    random.shuffle(ains)\n",
    "    ains = ains[:3]\n",
    "\n",
    "    print('Ains: ', ains, file=sys.stderr)\n",
    "    for ain in ains:\n",
    "        os.system('ffmpeg -y -loglevel error -i examples/wav_files/{} -ar 16000 examples/tmp.wav'.format(ain))\n",
    "        shutil.copyfile('examples/tmp.wav', 'examples/wav_files/{}'.format(ain))\n",
    "\n",
    "        # au embedding\n",
    "        from thirdparty.resemblyer_util.speaker_emb import get_spk_emb\n",
    "        me, ae = get_spk_emb('examples/wav_files/{}'.format(ain))\n",
    "        au_emb.append(me.reshape(-1))\n",
    "\n",
    "        print('Processing audio file', ain)\n",
    "        c = AutoVC_mel_Convertor('examples')\n",
    "\n",
    "        au_data_i = c.convert_single_wav_to_autovc_input(audio_filename=os.path.join('examples', 'wav_files', ain),\n",
    "               autovc_model_path=opt_parser.load_AUTOVC_name)\n",
    "        au_data += au_data_i\n",
    "    if(os.path.isfile('examples/tmp.wav')):\n",
    "        os.remove('examples/tmp.wav')\n",
    "\n",
    "    print(\"Loaded audio...\", file=sys.stderr)\n",
    "\n",
    "    # landmark fake placeholder\n",
    "    fl_data = []\n",
    "    rot_tran, rot_quat, anchor_t_shape = [], [], []\n",
    "    for au, info in au_data:\n",
    "        au_length = au.shape[0]\n",
    "        fl = np.zeros(shape=(au_length, 68 * 3))\n",
    "        fl_data.append((fl, info))\n",
    "        rot_tran.append(np.zeros(shape=(au_length, 3, 4)))\n",
    "        rot_quat.append(np.zeros(shape=(au_length, 4)))\n",
    "        anchor_t_shape.append(np.zeros(shape=(au_length, 68 * 3)))\n",
    "\n",
    "    if(os.path.exists(os.path.join('examples', 'dump', 'random_val_fl.pickle'))):\n",
    "        os.remove(os.path.join('examples', 'dump', 'random_val_fl.pickle'))\n",
    "    if(os.path.exists(os.path.join('examples', 'dump', 'random_val_fl_interp.pickle'))):\n",
    "        os.remove(os.path.join('examples', 'dump', 'random_val_fl_interp.pickle'))\n",
    "    if(os.path.exists(os.path.join('examples', 'dump', 'random_val_au.pickle'))):\n",
    "        os.remove(os.path.join('examples', 'dump', 'random_val_au.pickle'))\n",
    "    if (os.path.exists(os.path.join('examples', 'dump', 'random_val_gaze.pickle'))):\n",
    "        os.remove(os.path.join('examples', 'dump', 'random_val_gaze.pickle'))\n",
    "\n",
    "    with open(os.path.join('examples', 'dump', 'random_val_fl.pickle'), 'wb') as fp:\n",
    "        pickle.dump(fl_data, fp)\n",
    "    with open(os.path.join('examples', 'dump', 'random_val_au.pickle'), 'wb') as fp:\n",
    "        pickle.dump(au_data, fp)\n",
    "    with open(os.path.join('examples', 'dump', 'random_val_gaze.pickle'), 'wb') as fp:\n",
    "        gaze = {'rot_trans':rot_tran, 'rot_quat':rot_quat, 'anchor_t_shape':anchor_t_shape}\n",
    "        pickle.dump(gaze, fp)\n",
    "\n",
    "    model = Audio2landmark_model(opt_parser, jpg_shape=shape_3d)\n",
    "    if(len(opt_parser.reuse_train_emb_list) == 0):\n",
    "        model.test(au_emb=au_emb)\n",
    "    else:\n",
    "        model.test(au_emb=None)\n",
    "\n",
    "    print(\"Audio->Landmark...\", file=sys.stderr)\n",
    "\n",
    "    fls = glob.glob1('examples', 'pred_fls_*.txt')\n",
    "    fls.sort()\n",
    "    print('Fls: ', fls, file=sys.stderr)\n",
    "\n",
    "    for i in range(0,len(fls)):\n",
    "        fl = np.loadtxt(os.path.join('examples', fls[i])).reshape((-1, 68,3))\n",
    "        fl[:, :, 0:2] = -fl[:, :, 0:2]\n",
    "        fl[:, :, 0:2] = fl[:, :, 0:2] / scale - shift\n",
    "\n",
    "        if (ADD_NAIVE_EYE):\n",
    "            fl = util.add_naive_eye(fl)\n",
    "\n",
    "        # additional smooth\n",
    "        fl = fl.reshape((-1, 204))\n",
    "        fl[:, :48 * 3] = savgol_filter(fl[:, :48 * 3], 15, 3, axis=0)\n",
    "        fl[:, 48*3:] = savgol_filter(fl[:, 48*3:], 5, 3, axis=0)\n",
    "        fl = fl.reshape((-1, 68, 3))\n",
    "\n",
    "        ''' STEP 6: Imag2image translation '''\n",
    "        model = Image_translation_block(opt_parser, single_test=True)\n",
    "        with torch.no_grad():\n",
    "            model.single_test(jpg=img, fls=fl, filename=fls[i], prefix=opt_parser.jpg.split('.')[0])\n",
    "            print('finish image2image gen')\n",
    "        os.remove(os.path.join('examples', fls[i]))\n",
    "\n",
    "        print(\"{} / {}: Landmark->Face...\".format(i+1, len(fls)), file=sys.stderr)\n",
    "    print(\"Done!\", file=sys.stderr)\n",
    "\n",
    "    for ain in ains:\n",
    "          OUTPUT_MP4_NAME = '{}_pred_fls_{}_audio_embed.mp4'.format(\n",
    "              opt_parser.jpg.split('.')[0],\n",
    "              ain.split('.')[0]\n",
    "          )\n",
    "          mp4 = open('examples/{}'.format(OUTPUT_MP4_NAME),'rb').read()\n",
    "          data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "          print('Display animation: examples/{}'.format(OUTPUT_MP4_NAME), file=sys.stderr)\n",
    "          display(HTML(\"\"\"\n",
    "          <video width=600 controls>\n",
    "                <source src=\"%s\" type=\"video/mp4\">\n",
    "          </video>\n",
    "          \"\"\" % data_url))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApCZszX-CvuP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualize your animation!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qFBozkB1Cf8g",
    "outputId": "525045a3-069b-4600-f437-4ca669eb70d4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "for ain in ains:\n",
    "  OUTPUT_MP4_NAME = '{}_pred_fls_{}_audio_embed.mp4'.format(\n",
    "    opt_parser.jpg.split('.')[0],\n",
    "    ain.split('.')[0]\n",
    "    )\n",
    "  mp4 = open('examples/{}'.format(OUTPUT_MP4_NAME),'rb').read()\n",
    "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "  print('Display animation: examples/{}'.format(OUTPUT_MP4_NAME), file=sys.stderr)\n",
    "  display(HTML(\"\"\"\n",
    "  <video width=600 controls>\n",
    "        <source src=\"%s\" type=\"video/mp4\">\n",
    "  </video>\n",
    "  \"\"\" % data_url))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P0etBPyAC1e7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}